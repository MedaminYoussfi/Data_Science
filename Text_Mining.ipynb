{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MedaminYoussfi/Data_Science/blob/main/Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O__TQSpdubin",
        "outputId": "107123a3-2f84-4bce-a319-d62502eed12f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 200 characters :  Si vous vous intéressez au big data, vous connaissez certainement Apache Spark. Savez-vous pourquoi Spark est le framework de prédilection pour le traitement de données massives ? Pourquoi est-il auta\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "#lire les fichiers\n",
        "with open(os.getcwd()+\"/Essentiel-Apache-Spark.txt\", 'r') as fh:\n",
        "  filedata=fh.read()\n",
        "\n",
        "#afficher les 200 premiers caractères du fichier\n",
        "print(\"The first 200 characters : \", filedata[0:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lecture avec NLTK CorpusReader**"
      ],
      "metadata": {
        "id": "fC_7u-TGzD5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installer la bibliothéque nltk\n",
        "import nltk\n",
        "\n",
        "#télécharger le package punkt\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTXquKbexHXh",
        "outputId": "e510e79c-bdeb-4540-f15d-9f9a53a49257"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "\n",
        "#lire le fichier dans un corpus\n",
        "corpus=PlaintextCorpusReader(os.getcwd(), \"Essentiel-Apache-Spark.txt\")\n",
        "\n",
        "#afficher le contenu du corpus\n",
        "corpus.raw()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Sm_GP7WLxz6x",
        "outputId": "9692ca1c-a6ae-4da5-b335-70574ab30dac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Si vous vous intéressez au big data, vous connaissez certainement Apache Spark. Savez-vous pourquoi Spark est le framework de prédilection pour le traitement de données massives ? Pourquoi est-il autant apprécié notamment pour déployer les algorithmes de machine learning ? Découvrez ce cours sur Apache PySpark pour répondre à toutes vos questions. À travers de multiples exemples et mises en pratique, le professeur associé en technologies de l'information et techniques d'optimisation, vous donne toutes les clés pour analyser efficacement des données à grande échelle avec Apache Spark et Python.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explorer le corpus**"
      ],
      "metadata": {
        "id": "J5AFuo73zkni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extraire les IDs\n",
        "print(\"Fichier dans le crpus : \", corpus.fileids())\n",
        "\n",
        "#Extraire les paragraphes\n",
        "paragraphs=corpus.paras()\n",
        "print(\"\\n Nombre total de paragraphes corpus : \", len(paragraphs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCzD7_SIzsrV",
        "outputId": "73564f83-bc15-4197-ab48-9cb726fe538a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fichier dans le crpus :  ['Essentiel-Apache-Spark.txt']\n",
            "\n",
            " Nombre total de paragraphes corpus :  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extraire les phrases\n",
        "sentences=corpus.sents()\n",
        "print(\"\\n Nombre total de phrases corpus : \", len(sentences))\n",
        "\n",
        "#extraire la première phrase\n",
        "print(\"\\n Première phrase : \", sentences[0])\n",
        "\n",
        "#Extraire les mots\n",
        "print(\"\\n Mots dans le corpus : \", corpus.words )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reXPIWFC1J65",
        "outputId": "816e6b11-9d16-43a4-8376-ef4fd4739bc4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Nombre total de phrases corpus :  5\n",
            "\n",
            " Première phrase :  ['Si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', ',', 'vous', 'connaissez', 'certainement', 'Apache', 'Spark', '.']\n",
            "\n",
            " Mots dans le corpus :  <bound method PlaintextCorpusReader.words of <PlaintextCorpusReader in '/content'>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyser le corpus**"
      ],
      "metadata": {
        "id": "4EhcUc7E3st3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#La distribution de fréquence des mots dans le corpus\n",
        "course_freq_dist=nltk.FreqDist(corpus.words())\n",
        "\n",
        "#Afficher les mots les plus utilisés\n",
        "print(\"Top 10 des mots du corpus : \", course_freq_dist.most_common(10))\n",
        "\n",
        "#La distribution d'un mot spécifique\n",
        "print(\"\\n Distribution pour le mots \\\"Spark\\\" : \", course_freq_dist.get(\"Spark\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WxrBVGH3q9k",
        "outputId": "0edf3b18-78c2-4513-8362-b5e65791b376"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 des mots du corpus :  [('vous', 5), ('de', 5), ('pour', 4), (',', 3), ('Apache', 3), ('Spark', 3), ('.', 3), ('le', 3), ('et', 3), ('-', 2)]\n",
            "\n",
            " Distribution pour le mots \"Spark\" :  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenisation**"
      ],
      "metadata": {
        "id": "01z-B0s47J43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importation des bibliothéques\n",
        "import nltk\n",
        "import os\n",
        "\n",
        "#Télécharger le pachage punkt\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Lire le fichier de base\n",
        "base_file = open(os.getcwd()+ \"/Essentiel-Apache-Spark.txt\", 'rt')\n",
        "raw_text = base_file.read()\n",
        "base_file.close()\n",
        "\n",
        "#------------------------------------------------\n",
        "\n",
        "#Extraire des tokens\n",
        "token_list = nltk.wordpunct_tokenize(raw_text)\n",
        "print(\"Token List : \", token_list[:20])\n",
        "print(\"\\n Total Tokens : \", len(token_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TTDDkmA69ZM",
        "outputId": "da626217-d2f6-44ac-8c1d-fe429cf1ce2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token List :  ['Si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', ',', 'vous', 'connaissez', 'certainement', 'Apache', 'Spark', '.', 'Savez', '-', 'vous', 'pourquoi', 'Spark', 'est']\n",
            "\n",
            " Total Tokens :  103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nettoyage du texte**\n"
      ],
      "metadata": {
        "id": "bQ_TQD9T-ha-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- Supprimer la ponctuation**"
      ],
      "metadata": {
        "id": "d7ThGxwSAqlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extraire les jetons via la biblithéque Punkt\n",
        "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
        "print(\"Liste des tokens après suppression de la ponctuation : \", token_list2[:20])\n",
        "print(\"\\n Total des token après suppression de la ponctuation : \", len(token_list2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8hpsX0i-2yW",
        "outputId": "3fa58c63-614d-4c4e-f979-34bd289ac352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liste des tokens après suppression de la ponctuation :  ['Si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', 'vous', 'connaissez', 'certainement', 'Apache', 'Spark', 'Savez', 'vous', 'pourquoi', 'Spark', 'est', 'le', 'framework', 'de']\n",
            "\n",
            " Total des token après suppression de la ponctuation :  91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convertir en miniscules**"
      ],
      "metadata": {
        "id": "9NUbaJtjAvGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_list3 = [word.lower() for word in token_list2 ] #for word in token_list2\n",
        "                                                      #token_list3.append(word.lower())\n",
        "print(\"Liste des tokens après conversion en minuscules : \", token_list2[:20])\n",
        "print(\"Liste des tokens après conversion en minuscules : \", len(token_list2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B6qjTaAAz71",
        "outputId": "3277342d-fe22-4839-b7d1-1d3c89e98d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liste des tokens après conversion en minuscules :  ['Si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', 'vous', 'connaissez', 'certainement', 'Apache', 'Spark', 'Savez', 'vous', 'pourquoi', 'Spark', 'est', 'le', 'framework', 'de']\n",
            "Liste des tokens après conversion en minuscules :  91\n"
          ]
        }
      ]
    }
  ]
}